# Render.com deployment configuration
# Production-ready settings for RAG Model deployment

services:
  - type: web
    name: rag-model-api
    env: python
    plan: starter  # Upgrade to 'standard' to prevent spin-down
    region: oregon
    branch: main
    autoDeploy: false  # Prevent unnecessary deploys during evaluation
    
    # Health check configuration - helps Render keep service alive
    healthCheckPath: /health
    
    buildCommand: |
      # Render-optimized build process
      pip install --upgrade pip setuptools wheel
      pip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cpu
      pip install -r requirements.txt
      
      # Create cache directories
      mkdir -p /tmp/huggingface_cache /tmp/torch_cache
      
      # Pre-download models during build (faster startup)
      python -c "
      import os
      os.environ['RAG_RENDER_MODE'] = 'true'
      os.environ['HF_HOME'] = '/tmp/huggingface_cache'
      os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
      os.environ['TOKENIZERS_PARALLELISM'] = 'false'
      try:
          from model_cache import ModelCache
          ModelCache.get_embedder('all-MiniLM-L6-v2')
          print('✅ Models pre-downloaded successfully')
      except Exception as e:
          print(f'⚠️ Model pre-download failed: {e}')
          print('Models will download on first request')
      "
    
    startCommand: uvicorn main:app --host 0.0.0.0 --port $PORT --workers 1 --timeout-keep-alive 30
    
    envVars:
      - key: PYTHON_VERSION
        value: "3.12"
      - key: RAG_RENDER_MODE
        value: "true"
      - key: RAG_FAST_STARTUP
        value: "true"
      - key: RAG_PRELOAD_MODELS
        value: "true"
      - key: PYTORCH_ENABLE_MPS_FALLBACK
        value: "1"
      - key: TOKENIZERS_PARALLELISM
        value: "false"
      - key: HF_HOME
        value: "/tmp/huggingface_cache"
      - key: TORCH_HOME
        value: "/tmp/torch_cache"
      - key: RAG_PRIMARY_STORAGE
        value: "faiss"
      - key: RAG_MEMORY_OPTIMIZATION
        value: "true"
      - key: MAX_CONTEXT_LENGTH
        value: "800"
      - key: CHUNK_SIZE
        value: "4000"
      - key: TOP_K_RETRIEVAL
        value: "3"
      - key: PYTHONUNBUFFERED
        value: "1"
      - key: PYTHONDONTWRITEBYTECODE
        value: "1"
      - key: OMP_NUM_THREADS
        value: "1"
      - key: MKL_NUM_THREADS
        value: "1"
      - key: NUMEXPR_NUM_THREADS
        value: "1"
      - key: OPENBLAS_NUM_THREADS
        value: "1"
      
    # Health check configuration - keeps service responsive
    healthCheckPath: /health
